{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import renom as rm\n",
    "from renom.optimizer import Sgd, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0  batch:0 weight shape:(4, 10) bias shape:(1, 10)\n",
      "weight:[[-0.29827434  0.21000847  0.2092475   0.17629714  0.11775687  0.10078679\n",
      "  -0.16989267  0.434304   -0.46419695  0.41415146]\n",
      " [ 0.09677858  0.27148157 -0.83374274  0.53147715  0.33891159  0.10065224\n",
      "  -0.23715487 -0.04857498  0.17717342  0.02978326]\n",
      " [ 0.4142431  -0.39110917  0.31385189  0.20074128  0.0111355   0.03012815\n",
      "  -0.17707315  0.34429246  0.43995613 -0.13485508]\n",
      " [-0.07799219 -0.5901491  -0.25157365  0.23538105 -0.08147697 -0.65512246\n",
      "   0.13359053  0.39780629  0.11927841 -0.22154915]]\n",
      "bias:[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "epoch:0  batch:1 weight shape:(4, 10) bias shape:(1, 10)\n",
      "weight:[[-0.30009472  0.20991647  0.20902288  0.1763203   0.11769663  0.10093229\n",
      "  -0.16989267  0.43866056 -0.46381927  0.41211599]\n",
      " [ 0.09597535  0.27141944 -0.83383173  0.53122008  0.33852622  0.10079107\n",
      "  -0.23715487 -0.04669712  0.1773551   0.0289955 ]\n",
      " [ 0.4128378  -0.39113766  0.31367275  0.20131841  0.01184087  0.03007369\n",
      "  -0.17707315  0.34775224  0.44026211 -0.13665925]\n",
      " [-0.07845367 -0.59015298 -0.25164238  0.23565209 -0.0811363  -0.65515852\n",
      "   0.13359053  0.39895913  0.11937403 -0.2221767 ]]\n",
      "bias:[[-2.7555416e-04 -1.9338673e-05 -3.5837307e-05 -7.4793468e-05\n",
      "  -1.1458559e-04  4.6839701e-05  0.0000000e+00  6.4502907e-04\n",
      "   4.7809641e-05 -2.7506336e-04]]\n",
      "\n",
      "epoch:0  batch:2 weight shape:(4, 10) bias shape:(1, 10)\n",
      "weight:[[-0.30279854  0.20975214  0.20847541  0.17717829  0.11865088  0.10137588\n",
      "  -0.16989267  0.44605005 -0.46272472  0.40845275]\n",
      " [ 0.09470764  0.27130225 -0.8340742   0.53119516  0.33840868  0.10115093\n",
      "  -0.23715487 -0.04335101  0.1778875   0.02747486]\n",
      " [ 0.41062009 -0.3911829   0.31322849  0.20300245  0.01393359  0.03012542\n",
      "  -0.17707315  0.35388356  0.44119388 -0.14002325]\n",
      " [-0.07930255 -0.59016013 -0.25180653  0.23641784 -0.08017139 -0.65516484\n",
      "   0.13359053  0.40128031  0.11974271 -0.22349262]]\n",
      "bias:[[-6.9055264e-04 -5.1227329e-05 -1.2644354e-04 -1.8760435e-05\n",
      "  -6.9844667e-05  1.4024791e-04  0.0000000e+00  1.7747977e-03\n",
      "   2.1062800e-04 -8.0982712e-04]]\n",
      "\n",
      "epoch:000, train_loss:6.1533, test_loss:5.2987\n",
      "epoch:1  batch:0 weight shape:(4, 10) bias shape:(1, 10)\n",
      "weight:[[-0.32885167  0.20846947  0.20302522  0.19481488  0.13170552  0.10381439\n",
      "  -0.16989267  0.54549485 -0.4486818   0.3672339 ]\n",
      " [ 0.08256695  0.27040911 -0.83632648  0.5349831   0.33866608  0.10355698\n",
      "  -0.23715487  0.00164634  0.18434934  0.01022786]\n",
      " [ 0.38914549 -0.3915616   0.30871749  0.22710314  0.03774629  0.02954883\n",
      "  -0.17707315  0.43488392  0.45299825 -0.17685939]\n",
      " [-0.0870738  -0.59022772 -0.25332576  0.24631076 -0.06993508 -0.65551835\n",
      "   0.13359053  0.43031922  0.12414391 -0.2371114 ]]\n",
      "bias:[[-0.0046847  -0.00030561 -0.00091176  0.0017691   0.00066957  0.00074038\n",
      "   0.          0.01708768  0.00231906 -0.00683088]]\n",
      "\n",
      "epoch:1  batch:1 weight shape:(4, 10) bias shape:(1, 10)\n",
      "weight:[[-0.33003646  0.20837842  0.20266393  0.19588397  0.13179688  0.10392179\n",
      "  -0.16989267  0.55326897 -0.44777381  0.36474228]\n",
      " [ 0.08200917  0.27034447 -0.83647007  0.53503639  0.33812541  0.10371393\n",
      "  -0.23715487  0.00516412  0.1847681   0.00924098]\n",
      " [ 0.38814247 -0.39159074  0.30842164  0.22880392  0.03896112  0.0294574\n",
      "  -0.17707315  0.44115058  0.45375392 -0.17914268]\n",
      " [-0.08744652 -0.59023404 -0.25342074  0.24699832 -0.06938513 -0.65556002\n",
      "   0.13359053  0.43252835  0.1244211  -0.2379462 ]]\n",
      "bias:[[-0.00486938 -0.00032336 -0.00096548  0.00187745  0.0005952   0.00076674\n",
      "   0.          0.01834806  0.0024593  -0.00721263]]\n",
      "\n",
      "epoch:1  batch:2 weight shape:(4, 10) bias shape:(1, 10)\n",
      "weight:[[-0.33051038  0.20830134  0.2022613   0.19743571  0.13233472  0.10417803\n",
      "  -0.16989267  0.56140685 -0.44642535  0.3621254 ]\n",
      " [ 0.08178606  0.27029163 -0.83663809  0.53528661  0.33775702  0.10392813\n",
      "  -0.23715487  0.00881038  0.18538897  0.00821324]\n",
      " [ 0.38774124 -0.3916142   0.30810255  0.23111379  0.04082977  0.02945023\n",
      "  -0.17707315  0.44781715  0.45482937 -0.18162444]\n",
      " [-0.0875956  -0.59023857 -0.25353247  0.24796465 -0.06852817 -0.65557784\n",
      "   0.13359053  0.43493727  0.12481501 -0.23888578]]\n",
      "bias:[[-0.00494325 -0.00033872 -0.00102794  0.00204229  0.00056371  0.00082219\n",
      "   0.          0.01966242  0.00267636 -0.00760697]]\n",
      "\n",
      "epoch:001, train_loss:5.0824, test_loss:4.4965\n",
      "epoch:002, train_loss:4.1786, test_loss:3.8610\n",
      "epoch:003, train_loss:3.6687, test_loss:3.4668\n",
      "epoch:004, train_loss:3.3631, test_loss:3.3215\n",
      "epoch:005, train_loss:3.3226, test_loss:3.3137\n",
      "epoch:006, train_loss:3.3191, test_loss:3.3733\n",
      "epoch:007, train_loss:3.3787, test_loss:3.3171\n",
      "epoch:008, train_loss:3.3746, test_loss:3.3113\n",
      "epoch:009, train_loss:3.3135, test_loss:3.6049\n",
      "epoch:010, train_loss:3.3504, test_loss:3.5983\n",
      "epoch:011, train_loss:3.4395, test_loss:3.5872\n",
      "epoch:012, train_loss:5.5874, test_loss:19.3526\n",
      "epoch:013, train_loss:17.6368, test_loss:11.1493\n",
      "epoch:014, train_loss:24.8910, test_loss:36.6091\n",
      "epoch:015, train_loss:30.4534, test_loss:19.7268\n",
      "epoch:016, train_loss:31.0408, test_loss:36.8414\n",
      "epoch:017, train_loss:35.2067, test_loss:23.3869\n",
      "epoch:018, train_loss:31.0969, test_loss:29.6019\n",
      "epoch:019, train_loss:33.9228, test_loss:36.8414\n",
      "epoch:020, train_loss:36.4839, test_loss:36.8414\n",
      "epoch:021, train_loss:36.8164, test_loss:36.8414\n",
      "epoch:022, train_loss:36.4554, test_loss:36.8414\n",
      "epoch:023, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:024, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:025, train_loss:36.0969, test_loss:36.8414\n",
      "epoch:026, train_loss:37.1956, test_loss:36.0435\n",
      "epoch:027, train_loss:36.4307, test_loss:36.8414\n",
      "epoch:028, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:029, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:030, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:031, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:032, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:033, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:034, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:035, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:036, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:037, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:038, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:039, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:040, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:041, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:042, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:043, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:044, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:045, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:046, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:047, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:048, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:049, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:050, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:051, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:052, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:053, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:054, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:055, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:056, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:057, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:058, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:059, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:060, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:061, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:062, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:063, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:064, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:065, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:066, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:067, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:068, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:069, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:070, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:071, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:072, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:073, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:074, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:075, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:076, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:077, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:078, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:079, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:080, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:081, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:082, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:083, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:084, train_loss:36.4871, test_loss:36.8414\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:085, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:086, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:087, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:088, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:089, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:090, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:091, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:092, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:093, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:094, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:095, train_loss:36.8414, test_loss:36.8414\n",
      "epoch:096, train_loss:36.4871, test_loss:36.8414\n",
      "epoch:097, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:098, train_loss:37.1956, test_loss:36.8414\n",
      "epoch:099, train_loss:36.8414, test_loss:36.8414\n"
     ]
    }
   ],
   "source": [
    "class Model(rm.Model):\n",
    "    def __init__(self):\n",
    "        # 入力層のニューロンの数は自動的に推論される\n",
    "        # 中間層のニューロンが10\n",
    "        self.layer1 = rm.Dense(10)\n",
    "        # 出力層のニューロンが3 labelのカラムが3だから\n",
    "        self.layer2 = rm.Dense(3)\n",
    "\n",
    "    def forward(self, x, epoch, batch):\n",
    "        # 中間層の活性化関数はReLU\n",
    "        t1 = rm.relu(self.layer1(x))\n",
    "        out = self.layer2(t1)\n",
    "        \n",
    "        # Debug用\n",
    "        if epoch is not None and epoch < 2 and batch < 3:\n",
    "            print(\"epoch:{}  batch:{} weight shape:{} bias shape:{}\".format(epoch, batch, self.layer1.params.w.shape, self.layer1.params.b.shape))\n",
    "            print(\"weight:{}\".format(self.layer1.params.w))\n",
    "            print(\"bias:{}\".format(self.layer1.params.b))\n",
    "            print()\n",
    "        return out\n",
    "\n",
    "# Sequential Model にも同じような処理があり、そこに細かくコメントが記載してあるため、そちらを参照すること\n",
    "# https://seattle-consulting.qiita.com/maejimayuto/items/0db599ac052cd2679c40\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "label = iris.target\n",
    "\n",
    "model = Model()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3)\n",
    "# print(X_test)\n",
    "# print(y_test)\n",
    "# print(len(y_train))\n",
    "y_train = y_train.reshape(len(X_train), -1)\n",
    "y_test = y_test.reshape(len(X_test), -1)\n",
    "# print(y_train)\n",
    "batch_size = 8\n",
    "epoch = 100\n",
    "N = len(X_train)\n",
    "# print(len(X_train))\n",
    "optimizer = Sgd(lr=0.001)\n",
    "learning_curve = []\n",
    "test_learning_curve = []\n",
    "\n",
    "for i in range(epoch):\n",
    "#     print(N)\n",
    "    perm = np.random.permutation(N)\n",
    "#     perm = range(105)\n",
    "#     perm = np.sort(perm)\n",
    "#     print(perm)\n",
    "    loss = 0\n",
    "    for j in range(0, N // batch_size):\n",
    "#         print(perm[j*batch_size : (j+1)*batch_size])\n",
    "        train_batch = X_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "#         print(train_batch)\n",
    "        response_batch = y_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "\n",
    "#         print(\"1----------------\")\n",
    "#         a = model.forward(train_batch, i, j)\n",
    "#         print(\"a\")å\n",
    "#         print(type(a))\n",
    "#         print(a)\n",
    "#         print(\"2----------------\")\n",
    "#         なぜforwardで呼ばず、インスタンスの__call__の呼び出しをするのか\n",
    "        with model.train():\n",
    "            l = rm.softmax_cross_entropy(model(train_batch, i, j), response_batch)\n",
    "#             print(\"l:{}\".format(l))\n",
    "#             print(type(l))\n",
    "        grad = l.grad()\n",
    "        grad.update(optimizer)\n",
    "        loss += l.as_ndarray()\n",
    "    train_loss = loss / (N // batch_size)\n",
    "\n",
    "    test_loss = rm.softmax_cross_entropy(model(X_test, None, None), y_test).as_ndarray()\n",
    "    test_learning_curve.append(test_loss)\n",
    "    learning_curve.append(train_loss)\n",
    "    print(\"epoch:{:03d}, train_loss:{:.4f}, test_loss:{:.4f}\".format(i, float(train_loss), float(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 実行できないため、コメントアウト\n",
    "# 損失関数が現象してくことを確認したかった\n",
    "# predictions = np.argmax(model(X_test).as_ndarray(), axis=1)\n",
    "\n",
    "# print(confusion_matrix(y_test, predictions))\n",
    "# print(classification_report(y_test, predictions))\n",
    "\n",
    "# plt.plot(learning_curve, linewidth=1, label=\"train\")\n",
    "# plt.plot(test_learning_curve, linewidth=1, label=\"test\")\n",
    "# plt.title(\"learning_curve\")\n",
    "# plt.ylabel(\"error\")\n",
    "# plt.xlabel(\"epoch\")\n",
    "# plt.legend()\n",
    "# plt.grid()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# このマジックコメントがないとグラフは表示されない（by @azumag san）\n",
    "%matplotlib inline\n",
    "\n",
    "X, y = make_regression(n_samples=500, n_features=1, random_state=0, noise=4.0, bias=100.0)\n",
    "X = - X\n",
    "plt.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:000, train_loss:9896.2833, test_loss:7678.9565\n",
      "epoch:001, train_loss:3226.4067, test_loss:1839.8363\n",
      "epoch:002, train_loss:1784.2593, test_loss:1387.4137\n",
      "epoch:003, train_loss:1427.8430, test_loss:1026.4895\n",
      "epoch:004, train_loss:1220.3745, test_loss:891.5942\n",
      "epoch:005, train_loss:919.0171, test_loss:720.9926\n",
      "epoch:006, train_loss:718.2286, test_loss:647.1937\n",
      "epoch:007, train_loss:679.0971, test_loss:593.7420\n",
      "epoch:008, train_loss:646.2757, test_loss:627.7673\n",
      "epoch:009, train_loss:605.6660, test_loss:559.5913\n"
     ]
    }
   ],
   "source": [
    "class Model(rm.Model):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.w1 = rm.Variable(np.random.randn(input_size, hidden_size)*0.01)\n",
    "        self.w2 = rm.Variable(np.random.randn(hidden_size, output_size)*0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t1 = rm.dot(x, self.w1)\n",
    "        t2 = rm.relu(t1)\n",
    "        out = rm.dot(t2, self.w2)\n",
    "        return out\n",
    "\n",
    "data, label = make_regression(n_samples=500, n_features=5, random_state=0, noise=4.0, bias=100.0)\n",
    "data = - data\n",
    "\n",
    "model = Model(input_size=data.shape[1], hidden_size=10, output_size=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3)\n",
    "y_train = y_train.reshape(len(X_train), -1)\n",
    "y_test = y_test.reshape(len(X_test), -1)\n",
    "batch_size = 8\n",
    "epoch = 10\n",
    "N = len(X_train)\n",
    "optimizer = Sgd(lr=0.001)\n",
    "\n",
    "for i in range(epoch):\n",
    "    perm = np.random.permutation(N)\n",
    "    loss = 0\n",
    "    for j in range(0, N // batch_size):\n",
    "        train_batch = X_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "        response_batch = y_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "\n",
    "        with model.train():\n",
    "            l = rm.mean_squared_error(model(train_batch), response_batch)\n",
    "        grad = l.grad()\n",
    "        grad.update(optimizer)\n",
    "        loss += l.as_ndarray()\n",
    "    train_loss = loss / (N // batch_size)\n",
    "\n",
    "    test_loss = rm.mean_squared_error(model(X_test), y_test).as_ndarray()\n",
    "    print(\"epoch:{:03d}, train_loss:{:.4f}, test_loss:{:.4f}\".format(i, float(train_loss), float(test_loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:000, train_loss:7949.1930, test_loss:1279.4160\n",
      "epoch:001, train_loss:608.3815, test_loss:337.1844\n",
      "epoch:002, train_loss:558.6528, test_loss:328.5977\n",
      "epoch:003, train_loss:512.3202, test_loss:287.4895\n",
      "epoch:004, train_loss:366.3749, test_loss:109.9870\n",
      "epoch:005, train_loss:135.6646, test_loss:72.7759\n",
      "epoch:006, train_loss:95.3820, test_loss:71.4679\n",
      "epoch:007, train_loss:77.7823, test_loss:65.4372\n",
      "epoch:008, train_loss:72.2682, test_loss:59.4398\n",
      "epoch:009, train_loss:65.4304, test_loss:58.6941\n"
     ]
    }
   ],
   "source": [
    "class Model(rm.Model):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.w1 = rm.Variable(np.random.randn(input_size, hidden_size)*0.01)\n",
    "        self.b1 = rm.Variable(np.zeros((1, hidden_size)))\n",
    "        self.w2 = rm.Variable(np.random.randn(hidden_size, output_size)*0.01)\n",
    "        self.b2 = rm.Variable(np.zeros((1, output_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        t1 = rm.dot(x, self.w1) + self.b1\n",
    "        t2 = rm.relu(t1)\n",
    "        out = rm.dot(t2, self.w2) + self.b2\n",
    "        return out\n",
    "\n",
    "data, label = make_regression(n_samples=500, n_features=5, random_state=0, noise=4.0, bias=100.0)\n",
    "data = - data\n",
    "\n",
    "model = Model(input_size=data.shape[1], hidden_size=10, output_size=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.3)\n",
    "y_train = y_train.reshape(len(X_train), -1)\n",
    "y_test = y_test.reshape(len(X_test), -1)\n",
    "batch_size = 8\n",
    "epoch = 10\n",
    "N = len(X_train)\n",
    "optimizer = Sgd(lr=0.001)\n",
    "\n",
    "for i in range(epoch):\n",
    "    perm = np.random.permutation(N)\n",
    "    loss = 0\n",
    "    for j in range(0, N // batch_size):\n",
    "        train_batch = X_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "        response_batch = y_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "\n",
    "        with model.train():\n",
    "            l = rm.mean_squared_error(model(train_batch), response_batch)\n",
    "        grad = l.grad()\n",
    "        grad.update(optimizer)\n",
    "        loss += l.as_ndarray()\n",
    "    train_loss = loss / (N // batch_size)\n",
    "\n",
    "    test_loss = rm.mean_squared_error(model(X_test), y_test).as_ndarray()\n",
    "    print(\"epoch:{:03d}, train_loss:{:.4f}, test_loss:{:.4f}\".format(i, float(train_loss), float(test_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
