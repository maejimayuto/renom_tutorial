{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import renom as rm\n",
    "from renom.optimizer import Sgd, Adam\n",
    "from renom.cuda.cuda import set_cuda_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_cuda_active(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./cifar-10-batches-py/\"\n",
    "paths = [\"data_batch_1\", \"data_batch_2\", \"data_batch_3\",\n",
    "         \"data_batch_4\", \"data_batch_5\"]\n",
    "\n",
    "def unpickle(f):\n",
    "    fo = open(f, 'rb')\n",
    "    if sys.version_info.major == 2:\n",
    "        # Python 2.7\n",
    "        d = pickle.load(fo)\n",
    "    elif sys.version_info.major == 3:\n",
    "        # Python 3.4\n",
    "        d = pickle.load(fo, encoding=\"latin-1\")\n",
    "    fo.close()\n",
    "    return d\n",
    "\n",
    "# Load train data.\n",
    "data = list(map(unpickle, [os.path.join(dir, p) for p in paths]))\n",
    "train_x = np.vstack([d[\"data\"] for d in data])\n",
    "train_y = np.vstack([d[\"labels\"] for d in data])\n",
    "\n",
    "# Load test data.\n",
    "data = unpickle(os.path.join(dir, \"test_batch\"))\n",
    "test_x = np.array(data[\"data\"])\n",
    "test_y = np.array(data[\"labels\"])\n",
    "\n",
    "# Rehsape and rescale image.\n",
    "train_x = train_x.reshape(-1, 3, 32, 32)\n",
    "train_y = train_y.reshape(-1, 1)\n",
    "test_x = test_x.reshape(-1, 3, 32, 32)\n",
    "test_y = test_y.reshape(-1, 1)\n",
    "\n",
    "train_x = train_x / 255.\n",
    "test_x = test_x / 255.\n",
    "\n",
    "# Binalize\n",
    "labels_train = LabelBinarizer().fit_transform(train_y)\n",
    "labels_test = LabelBinarizer().fit_transform(test_y)\n",
    "\n",
    "# Change types.\n",
    "train_x = train_x.astype(np.float32)\n",
    "test_x = test_x.astype(np.float32)\n",
    "labels_train = labels_train.astype(np.float32)\n",
    "labels_test = labels_test.astype(np.float32)\n",
    "\n",
    "N = len(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'cifar10/batches.meta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-519cec8e4e40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ラベル名をロード\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlabel_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cifar10/batches.meta\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cifar10/data_batch_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-904d02c904bd>\u001b[0m in \u001b[0;36munpickle\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0munpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmajor\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;31m# Python 2.7\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'cifar10/batches.meta'"
     ]
    }
   ],
   "source": [
    "# ラベル名をロード\n",
    "label_names = unpickle(\"cifar-10-batches-py/batches.meta\")[\"label_names\"]\n",
    "d = unpickle(\"cifar-10-batches-py/data_batch_1\")\n",
    "data = d[\"data\"]\n",
    "labels = np.array(d[\"labels\"])\n",
    "nsamples = len(data)\n",
    "\n",
    "print(label_names)\n",
    "\n",
    "# 各クラスの画像をランダムに10枚抽出して描画\n",
    "nclasses = 10\n",
    "pos = 1\n",
    "for i in range(nclasses):\n",
    "    # クラスiの画像のインデックスリストを取得\n",
    "    targets = np.where(labels == i)[0]\n",
    "    np.random.shuffle(targets)\n",
    "    # 最初の10枚の画像を描画\n",
    "    for idx in targets[:10]:\n",
    "        plt.subplot(10, 10, pos)\n",
    "        img = data[idx]\n",
    "        # (channel, row, column) => (row, column, channel)\n",
    "        plt.imshow(img.reshape(3, 32, 32).transpose(1, 2, 0))\n",
    "        plt.axis('off')\n",
    "        label = label_names[i]\n",
    "        pos += 1\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10(rm.Model):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Cifar10, self).__init__()\n",
    "        self._l1 = rm.Conv2d(channel=32)\n",
    "        self._l2 = rm.Conv2d(channel=32)\n",
    "        self._l3 = rm.Conv2d(channel=64)\n",
    "        self._l4 = rm.Conv2d(channel=64)\n",
    "        self._l5 = rm.Dense(512)\n",
    "        self._l6 = rm.Dense(10)\n",
    "        self._sd = rm.SpatialDropout(dropout_ratio=0.25)\n",
    "        self._pool = rm.MaxPool2d(filter=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        t1 = rm.relu(self._l1(x))\n",
    "        t2 = self._sd(self._pool(rm.relu(self._l2(t1))))\n",
    "        t3 = rm.relu(self._l3(t2))\n",
    "        t4 = self._sd(self._pool(rm.relu(self._l4(t3))))\n",
    "        t5 = rm.flatten(t4)\n",
    "        t6 = rm.dropout(rm.relu(self._l5(t5)))\n",
    "        t7 = self._l6(t5)\n",
    "        return t7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential = rm.Sequential([\n",
    "        rm.Conv2d(channel=32),\n",
    "        rm.BatchNormalize(),\n",
    "        rm.Relu(),\n",
    "        rm.Conv2d(channel=32),\n",
    "        rm.BatchNormalize(),\n",
    "        rm.Relu(),\n",
    "        rm.MaxPool2d(filter=2, stride=2),\n",
    "        rm.Dropout(dropout_ratio=0.25),\n",
    "        rm.Conv2d(channel=64),\n",
    "        rm.BatchNormalize(),\n",
    "        rm.Relu(),\n",
    "        rm.Conv2d(channel=64),\n",
    "        rm.BatchNormalize(),\n",
    "        rm.Relu(),\n",
    "        rm.MaxPool2d(filter=2, stride=2),\n",
    "        rm.Dropout(dropout_ratio=0.25),\n",
    "        rm.Flatten(),\n",
    "        rm.Dense(512),\n",
    "        rm.Relu(),\n",
    "        rm.Dropout(dropout_ratio=0.5),\n",
    "        rm.Dense(10),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose neural network.\n",
    "# network = Cifar10()\n",
    "network = sequential\n",
    "optimizer = Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "batch = 128\n",
    "epoch = 20\n",
    "\n",
    "learning_curve = []\n",
    "test_learning_curve = []\n",
    "\n",
    "for i in range(epoch):\n",
    "    perm = np.random.permutation(N)\n",
    "    loss = 0\n",
    "    for j in range(0, N // batch):\n",
    "        train_batch = train_x[perm[j * batch:(j + 1) * batch]]\n",
    "        responce_batch = labels_train[perm[j * batch:(j + 1) * batch]]\n",
    "\n",
    "        # Loss function\n",
    "        network.set_models(inference=False)\n",
    "        with network.train():\n",
    "            l = rm.softmax_cross_entropy(network(train_batch), responce_batch)\n",
    "\n",
    "        # Back propagation\n",
    "        grad = l.grad()\n",
    "\n",
    "        # Update\n",
    "        grad.update(optimizer)\n",
    "        loss += l.as_ndarray()\n",
    "\n",
    "    train_loss = loss / (N // batch)\n",
    "\n",
    "    # Validation\n",
    "    test_loss = 0\n",
    "    M = len(test_x)\n",
    "    network.set_models(inference=True)\n",
    "    for j in range(M//batch):\n",
    "        test_batch = test_x[j * batch:(j + 1) * batch]\n",
    "        test_label_batch = labels_test[j * batch:(j + 1) * batch]\n",
    "        prediction = network(test_batch)\n",
    "        test_loss += rm.softmax_cross_entropy(prediction, test_label_batch).as_ndarray()\n",
    "    test_loss /= (j+1)\n",
    "\n",
    "    test_learning_curve.append(test_loss)\n",
    "    learning_curve.append(train_loss)\n",
    "    print(\"epoch %03d train_loss:%f test_loss:%f\"%(i, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network.set_models(inference=True)\n",
    "predictions = np.argmax(network(test_x).as_ndarray(), axis=1)\n",
    "\n",
    "# Confusion matrix and classification report.\n",
    "print(confusion_matrix(test_y, predictions))\n",
    "print(classification_report(test_y, predictions))\n",
    "\n",
    "# Learning curve.\n",
    "plt.plot(learning_curve, linewidth=3, label=\"train\")\n",
    "plt.plot(test_learning_curve, linewidth=3, label=\"test\")\n",
    "plt.title(\"Learning curve\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
