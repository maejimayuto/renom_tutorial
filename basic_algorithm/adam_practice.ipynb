{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import renom as rm\n",
    "from renom.cuda.cuda import set_cuda_active\n",
    "set_cuda_active(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./isolet1+2+3+4.data\"\n",
    "labels = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "def make_label_idx(filename):\n",
    "    labels = []\n",
    "    for line in open(filename, \"r\"):\n",
    "        line = line.rstrip()\n",
    "        label = line.split(\",\")[-1]\n",
    "        labels.append(label)\n",
    "    labels = list(set(labels))\n",
    "    return list(set(labels))\n",
    "\n",
    "labels = make_label_idx(filename)\n",
    "labels = sorted(labels, key=lambda d:int(d.replace(\".\",\"\").replace(\" \",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(4990, 617), y_train:(4990,), X_test:(1248, 617), y_test:(1248,)\n",
      "labels_train:(4990, 26), labels_test:(1248, 26)\n"
     ]
    }
   ],
   "source": [
    "for line in open(filename,\"r\"):\n",
    "    line = line.rstrip()\n",
    "    label = labels.index(line.split(\",\")[-1])\n",
    "    features = list(map(float,line.split(\",\")[:-1]))\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"X_train:{}, y_train:{}, X_test:{}, y_test:{}\".format(X_train.shape, y_train.shape,\n",
    "                                                            X_test.shape, y_test.shape))\n",
    "lb = LabelBinarizer().fit(y)\n",
    "labels_train = lb.transform(y_train)\n",
    "labels_test = lb.transform(y_test)\n",
    "print(\"labels_train:{}, labels_test:{}\".format(labels_train.shape, labels_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(labels)\n",
    "sequential = rm.Sequential([\n",
    "    rm.Dense(100),\n",
    "    rm.Relu(),\n",
    "    rm.Dense(50),\n",
    "    rm.Relu(),\n",
    "    rm.Dense(output_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:000, train_loss:1.3317, test_loss:0.4834\n",
      "epoch:001, train_loss:0.3664, test_loss:0.3139\n",
      "epoch:002, train_loss:0.2488, test_loss:0.2133\n",
      "epoch:003, train_loss:0.1843, test_loss:0.3111\n",
      "epoch:004, train_loss:0.1542, test_loss:0.2365\n",
      "epoch:005, train_loss:0.1022, test_loss:0.1880\n",
      "epoch:006, train_loss:0.1007, test_loss:0.1825\n",
      "epoch:007, train_loss:0.0902, test_loss:0.1663\n",
      "epoch:008, train_loss:0.0653, test_loss:0.1451\n",
      "epoch:009, train_loss:0.0524, test_loss:0.3906\n",
      "epoch:010, train_loss:0.0692, test_loss:0.1800\n",
      "epoch:011, train_loss:0.0621, test_loss:0.2338\n",
      "epoch:012, train_loss:0.1053, test_loss:0.2653\n",
      "epoch:013, train_loss:0.0978, test_loss:0.2123\n",
      "epoch:014, train_loss:0.0598, test_loss:0.2020\n",
      "epoch:015, train_loss:0.0470, test_loss:0.2419\n",
      "epoch:016, train_loss:0.0420, test_loss:0.2790\n",
      "epoch:017, train_loss:0.0339, test_loss:0.2289\n",
      "epoch:018, train_loss:0.0206, test_loss:0.2019\n",
      "epoch:019, train_loss:0.0132, test_loss:0.2602\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "batch_size = 128\n",
    "N = len(X_train)\n",
    "optimizer = rm.Adam(lr=0.01)\n",
    "for i in range(epoch):\n",
    "    perm = np.random.permutation(N)\n",
    "    loss = 0\n",
    "    for j in range(0, N//batch_size):\n",
    "        train_batch = X_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "        response_batch = labels_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "        with sequential.train():\n",
    "            l = rm.softmax_cross_entropy(sequential(train_batch), response_batch)\n",
    "        grad = l.grad()\n",
    "        grad.update(optimizer)\n",
    "        loss += l.as_ndarray()\n",
    "    train_loss = loss / (N//batch_size)\n",
    "    test_loss = rm.softmax_cross_entropy(sequential(X_test), labels_test).as_ndarray()\n",
    "    print(\"epoch:{:03d}, train_loss:{:.4f}, test_loss:{:.4f}\".format(i, float(train_loss), float(test_loss)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
