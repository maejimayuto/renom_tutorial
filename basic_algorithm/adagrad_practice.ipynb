{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import renom as rm\n",
    "from renom.cuda.cuda import set_cuda_active\n",
    "set_cuda_active(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"./isolet1+2+3+4.data\"\n",
    "labels = []\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "def make_label_idx(filename):\n",
    "    labels = []\n",
    "    for line in open(filename, \"r\"):\n",
    "        line = line.rstrip()\n",
    "        label = line.split(\",\")[-1]\n",
    "        labels.append(label)\n",
    "    labels = list(set(labels))\n",
    "    return list(set(labels))\n",
    "\n",
    "labels = make_label_idx(filename)\n",
    "labels = sorted(labels, key=lambda d:int(d.replace(\".\",\"\").replace(\" \",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:(4990, 617), y_train:(4990,), X_test:(1248, 617), y_test:(1248,)\n",
      "labels_train:(4990, 26), labels_test:(1248, 26)\n"
     ]
    }
   ],
   "source": [
    "for line in open(filename,\"r\"):\n",
    "    line = line.rstrip()\n",
    "    label = labels.index(line.split(\",\")[-1])\n",
    "    features = list(map(float,line.split(\",\")[:-1]))\n",
    "    X.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "print(\"X_train:{}, y_train:{}, X_test:{}, y_test:{}\".format(X_train.shape, y_train.shape,\n",
    "                                                            X_test.shape, y_test.shape))\n",
    "lb = LabelBinarizer().fit(y)\n",
    "labels_train = lb.transform(y_train)\n",
    "labels_test = lb.transform(y_test)\n",
    "print(\"labels_train:{}, labels_test:{}\".format(labels_train.shape, labels_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = len(labels)\n",
    "sequential = rm.Sequential([\n",
    "    rm.Dense(100),\n",
    "    rm.Relu(),\n",
    "    rm.Dense(50),\n",
    "    rm.Relu(),\n",
    "    rm.Dense(output_size)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:000, train_loss:1.4529, test_loss:0.6893\n",
      "epoch:001, train_loss:0.5232, test_loss:0.4279\n",
      "epoch:002, train_loss:0.3581, test_loss:0.3612\n",
      "epoch:003, train_loss:0.2839, test_loss:0.3005\n",
      "epoch:004, train_loss:0.2332, test_loss:0.2766\n",
      "epoch:005, train_loss:0.2021, test_loss:0.2508\n",
      "epoch:006, train_loss:0.1784, test_loss:0.2332\n",
      "epoch:007, train_loss:0.1613, test_loss:0.2256\n",
      "epoch:008, train_loss:0.1433, test_loss:0.2157\n",
      "epoch:009, train_loss:0.1367, test_loss:0.2093\n",
      "epoch:010, train_loss:0.1215, test_loss:0.2021\n",
      "epoch:011, train_loss:0.1126, test_loss:0.1979\n",
      "epoch:012, train_loss:0.1049, test_loss:0.1994\n",
      "epoch:013, train_loss:0.0990, test_loss:0.1996\n",
      "epoch:014, train_loss:0.0936, test_loss:0.1869\n",
      "epoch:015, train_loss:0.0888, test_loss:0.1844\n",
      "epoch:016, train_loss:0.0834, test_loss:0.1863\n",
      "epoch:017, train_loss:0.0787, test_loss:0.1808\n",
      "epoch:018, train_loss:0.0747, test_loss:0.1763\n",
      "epoch:019, train_loss:0.0707, test_loss:0.1774\n"
     ]
    }
   ],
   "source": [
    "epoch = 20\n",
    "batch_size = 128\n",
    "N = len(X_train)\n",
    "optimizer = rm.Adagrad(lr=0.01)\n",
    "for i in range(epoch):\n",
    "    perm = np.random.permutation(N)\n",
    "    loss = 0\n",
    "    for j in range(0, N//batch_size):\n",
    "        train_batch = X_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "        response_batch = labels_train[perm[j*batch_size : (j+1)*batch_size]]\n",
    "        with sequential.train():\n",
    "            l = rm.softmax_cross_entropy(sequential(train_batch), response_batch)\n",
    "        grad = l.grad()\n",
    "        grad.update(optimizer)\n",
    "        loss += l.as_ndarray()\n",
    "    train_loss = loss / (N//batch_size)\n",
    "    test_loss = rm.softmax_cross_entropy(sequential(X_test), labels_test).as_ndarray()\n",
    "    print(\"epoch:{:03d}, train_loss:{:.4f}, test_loss:{:.4f}\".format(i, float(train_loss), float(test_loss)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
